{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# **Project Goal**\n\nThe primary goal of this project is to systematically evaluate the impact of various preprocessing techniques and machine learning models on the performance of a predictive model. Specifically, I aim to:\n\n1. **Identify the Best Scaling Method**: Determine which scaling technique (e.g., StandardScaler, MinMaxScaler, or no scaling) optimally prepares the data for model training.\n   \n2. **Determine the Most Effective Class Imbalance Handling Strategy**: Explore different methods for managing class imbalance, such as using SMOTE (Synthetic Minority Over-sampling Technique), class weights, or no imbalance handling, to enhance the modelâ€™s ability to predict minority class instances.\n   \n3. **Evaluate and Compare Machine Learning Models**: Assess the performance of various models, including Logistic Regression, Random Forest, and Hist Gradient Boosting, to identify the most suitable model for this dataset.\n\nBy keeping all other parameters constant while varying one component at a time, this approach will enable a clear understanding of how each preprocessing step and model choice affects the overall performance. This systematic evaluation will not only help in selecting the best-performing model but also provide insights into the role of each method in the machine learning pipeline.\n\n---","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import cross_val_score , KFold, train_test_split\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-31T18:36:44.709744Z","iopub.execute_input":"2024-08-31T18:36:44.710311Z","iopub.status.idle":"2024-08-31T18:36:44.717886Z","shell.execute_reply.started":"2024-08-31T18:36:44.71026Z","shell.execute_reply":"2024-08-31T18:36:44.716542Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Loading**","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:51:53.223262Z","iopub.execute_input":"2024-08-31T17:51:53.224335Z","iopub.status.idle":"2024-08-31T17:51:55.815736Z","shell.execute_reply.started":"2024-08-31T17:51:53.224276Z","shell.execute_reply":"2024-08-31T17:51:55.814596Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Exploratory Data Analysis (EDA)**","metadata":{}},{"cell_type":"code","source":"df.shape # 30 features and 284807 samples","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:51:55.817986Z","iopub.execute_input":"2024-08-31T17:51:55.818478Z","iopub.status.idle":"2024-08-31T17:51:55.826886Z","shell.execute_reply.started":"2024-08-31T17:51:55.818424Z","shell.execute_reply":"2024-08-31T17:51:55.825578Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info() # no missing values and all features are numarical","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:51:55.828232Z","iopub.execute_input":"2024-08-31T17:51:55.828594Z","iopub.status.idle":"2024-08-31T17:51:55.862017Z","shell.execute_reply.started":"2024-08-31T17:51:55.828559Z","shell.execute_reply":"2024-08-31T17:51:55.860801Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe().iloc[:,:10] # by only seeing the first 10 columns features its clear that there is great varity in the range of the features so scalling will be needed","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:51:55.864565Z","iopub.execute_input":"2024-08-31T17:51:55.864948Z","iopub.status.idle":"2024-08-31T17:51:56.328762Z","shell.execute_reply.started":"2024-08-31T17:51:55.864909Z","shell.execute_reply":"2024-08-31T17:51:56.327589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.nunique()  # all features have a variety of unique values, no transformation to object type is needed.","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:51:56.330359Z","iopub.execute_input":"2024-08-31T17:51:56.330851Z","iopub.status.idle":"2024-08-31T17:51:56.80957Z","shell.execute_reply.started":"2024-08-31T17:51:56.330799Z","shell.execute_reply":"2024-08-31T17:51:56.808345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.catplot(x=\"Class\",data=df,kind=\"count\")\nplt.show() \n# The data is greatly imbalanced, with class 0 being very low, so we will need to apply techniques to handle this.\n# Recommended order for handling imbalanced data:\n# 1. Start by setting class weights in your models to handle imbalance without altering the data.\n# 2. If class weights alone aren't sufficient, apply SMOTE to generate synthetic samples for the minority class.\n# 3. Evaluate model performance using appropriate metrics like F1-score and ROC AUC throughout the process.\n# 4. Consider using ensemble methods like Balanced Random Forest if class weights and SMOTE do not yield satisfactory results.\n# 5. Use stratified cross-validation to maintain class distribution across folds for reliable evaluation.\n# 6. Finally, fine-tune the model by adjusting the decision threshold based on your evaluation metrics to optimize the trade-off between precision and recall.\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:51:56.81098Z","iopub.execute_input":"2024-08-31T17:51:56.811338Z","iopub.status.idle":"2024-08-31T17:51:57.165887Z","shell.execute_reply.started":"2024-08-31T17:51:56.811299Z","shell.execute_reply":"2024-08-31T17:51:57.164652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),annot=True,fmt=\"0.02f\",cmap=\"Blues\")\nplt.show()\n# 'Amount' and 'Time' columns have high correlation with other features, which could introduce multicollinearity and affect model performance.\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:51:57.167318Z","iopub.execute_input":"2024-08-31T17:51:57.167695Z","iopub.status.idle":"2024-08-31T17:52:01.147213Z","shell.execute_reply.started":"2024-08-31T17:51:57.167649Z","shell.execute_reply":"2024-08-31T17:52:01.145939Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Insights**\n* we need to perform scalling \n* we need to handle class imabalanced \n* we need to drop amount and time feature ","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"Amount\",\"Time\"],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:52:01.148842Z","iopub.execute_input":"2024-08-31T17:52:01.149228Z","iopub.status.idle":"2024-08-31T17:52:01.178161Z","shell.execute_reply.started":"2024-08-31T17:52:01.149189Z","shell.execute_reply":"2024-08-31T17:52:01.176888Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(df.drop(\"Class\",axis=1),df[[\"Class\"]],shuffle=True,stratify=df[[\"Class\"]],test_size=0.2)\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:52:01.179678Z","iopub.execute_input":"2024-08-31T17:52:01.180096Z","iopub.status.idle":"2024-08-31T17:52:02.527667Z","shell.execute_reply.started":"2024-08-31T17:52:01.180054Z","shell.execute_reply":"2024-08-31T17:52:02.526209Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import StratifiedKFold\n# kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=8)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:52:02.53126Z","iopub.execute_input":"2024-08-31T17:52:02.531955Z","iopub.status.idle":"2024-08-31T17:52:02.537449Z","shell.execute_reply.started":"2024-08-31T17:52:02.531908Z","shell.execute_reply":"2024-08-31T17:52:02.536191Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **1. Baseline Pipeline**\n   - **Components**: No Scaling, No Imbalance Handling, Simple Model (e.g., Logistic Regression)\n   - **Purpose**: Establish a baseline performance to compare with other pipelines.\n","metadata":{}},{"cell_type":"code","source":"# Define the baseline pipeline\nbase_pipeline = Pipeline([\n    (\"model\", LogisticRegression())\n])\n\n# Evaluate the model\nbase_pipeline.fit(x_train,y_train.values.reshape(-1)) \ny_pred=base_pipeline.predict(x_test)\n\nprint(classification_report(y_test,y_pred))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:52:02.539223Z","iopub.execute_input":"2024-08-31T17:52:02.540136Z","iopub.status.idle":"2024-08-31T17:52:04.697637Z","shell.execute_reply.started":"2024-08-31T17:52:02.540081Z","shell.execute_reply":"2024-08-31T17:52:04.696416Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **2. Scaling Pipelines**\n   - **Pipeline 1: Standard Scaling**\n     - **Components**: StandardScaler for scaling, No Imbalance Handling, Simple Model (e.g., Logistic Regression)\n     - **Purpose**: Evaluate the effect of standard scaling on model performance.\n   - **Pipeline 2: MinMax Scaling**\n     - **Components**: MinMaxScaler for scaling, No Imbalance Handling, Simple Model (e.g., Logistic Regression)\n     - **Purpose**: Evaluate the effect of min-max scaling on model performance.","metadata":{}},{"cell_type":"code","source":"# Define the baseline pipeline\nstanderd_scaler_pipeline = Pipeline([\n    (\"standard scaling\",StandardScaler()),\n    (\"model\", LogisticRegression())\n])\n\n# Evaluate the model\nstanderd_scaler_pipeline.fit(x_train,y_train.values.reshape(-1)) \ny_pred=standerd_scaler_pipeline.predict(x_test)\n\nprint(classification_report(y_test,y_pred))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:52:43.653022Z","iopub.execute_input":"2024-08-31T17:52:43.653606Z","iopub.status.idle":"2024-08-31T17:52:45.622935Z","shell.execute_reply.started":"2024-08-31T17:52:43.65355Z","shell.execute_reply":"2024-08-31T17:52:45.621816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the baseline pipeline\nminmax_scaler_pipeline = Pipeline([\n    (\"standard scaling\",MinMaxScaler()),\n    (\"model\", LogisticRegression())\n])\n\n# Evaluate the model\nminmax_scaler_pipeline.fit(x_train,y_train.values.reshape(-1)) \ny_pred=minmax_scaler_pipeline.predict(x_test)\n\nprint(classification_report(y_test,y_pred))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:53:15.58348Z","iopub.execute_input":"2024-08-31T17:53:15.583917Z","iopub.status.idle":"2024-08-31T17:53:18.182053Z","shell.execute_reply.started":"2024-08-31T17:53:15.583879Z","shell.execute_reply":"2024-08-31T17:53:18.180627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **3. Imbalance Handling Pipelines**\n   - **Pipeline 3: Standard Scaling + SMOTE**\n     - **Components**: StandardScaler for scaling, SMOTE for imbalance handling, Simple Model (e.g., Logistic Regression)\n     - **Purpose**: Evaluate the effect of SMOTE on a scaled dataset.\n   - **Pipeline 4: Standard Scaling + Class Weights**\n     - **Components**: StandardScaler for scaling, Class Weights for imbalance handling, Simple Model (e.g., Logistic Regression)\n     - **Purpose**: Evaluate the effect of using class weights on a scaled dataset.","metadata":{}},{"cell_type":"code","source":"# Define the baseline pipeline\nclass_weight_pipeline = Pipeline([\n    (\"standard scaling\",StandardScaler()),\n    (\"model\", LogisticRegression(class_weight=\"balanced\"))\n])\n\n# Evaluate the model\nclass_weight_pipeline.fit(x_train,y_train.values.reshape(-1)) \ny_pred=class_weight_pipeline.predict(x_test)\n\nprint(classification_report(y_test,y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:54:50.429112Z","iopub.execute_input":"2024-08-31T17:54:50.42954Z","iopub.status.idle":"2024-08-31T17:54:51.831571Z","shell.execute_reply.started":"2024-08-31T17:54:50.4295Z","shell.execute_reply":"2024-08-31T17:54:51.830215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the pipeline\nsmote_pipeline = Pipeline([\n    (\"standard scaling\", StandardScaler()),\n    (\"smote\", SMOTE()), \n    (\"model\", LogisticRegression())\n])\n\n# Fit the pipeline on the full training set\nsmote_pipeline.fit(x_train, y_train.values.reshape(-1))\n\n# Predict on the test set\ny_pred = smote_pipeline.predict(x_test)\n\n# Print the classification report for the test set\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T18:01:59.923157Z","iopub.execute_input":"2024-08-31T18:01:59.923586Z","iopub.status.idle":"2024-08-31T18:02:03.102899Z","shell.execute_reply.started":"2024-08-31T18:01:59.923546Z","shell.execute_reply":"2024-08-31T18:02:03.101667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4. Model Evaluation Pipelines**\n\n   - **Pipeline 6: Standard Scaling + Class Balanced + Random Forest**\n     - **Components**: Best Scaling Method, Best Imbalance Handling Method, Random Forest Model\n     - **Purpose**: Test the performance of Random Forest using the best identified preprocessing methods.\n\n   - **Pipeline 7: Standard Scaling + Class Balanced + AdaBoost**\n     - **Components**: Best Scaling Method, Best Imbalance Handling Method, AdaBoost Model with a base `DecisionTreeClassifier` (using `class_weight=\"balanced\"` for the base estimator)\n     - **Purpose**: Test the performance of AdaBoost with a decision tree base estimator that handles class imbalance.\n\n   - **Pipeline 8: Standard Scaling + Class Balanced + Hist Gradient Boosting Classifier**\n     - **Components**: Components: Best Scaling Method, class_weight=\"balanced\" in HistGradientBoostingClassifier\n     - **Purpose**: Test the performance of HistGradientBoostingClassifier, which natively supports class balancing, using the best identified preprocessing methods.","metadata":{}},{"cell_type":"code","source":"# Define the baseline pipeline\nrandom_forest_pipeline = Pipeline([\n    (\"standard scaling\",StandardScaler()),\n    (\"model\", RandomForestClassifier(class_weight=\"balanced\"))\n])\n\n# Evaluate the model\nrandom_forest_pipeline.fit(x_train,y_train.values.reshape(-1)) \ny_pred=random_forest_pipeline.predict(x_test)\n\nprint(classification_report(y_test,y_pred))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T18:06:15.853973Z","iopub.execute_input":"2024-08-31T18:06:15.854416Z","iopub.status.idle":"2024-08-31T18:09:03.474195Z","shell.execute_reply.started":"2024-08-31T18:06:15.854362Z","shell.execute_reply":"2024-08-31T18:09:03.472624Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_estimator = DecisionTreeClassifier(class_weight=\"balanced\")\n# Define the baseline pipeline\nadav_boosting_pipeline = Pipeline([\n    (\"standard scaling\",StandardScaler()),\n    (\"model\", AdaBoostClassifier(base_estimator))\n])\n\n# Evaluate the model\nadav_boosting_pipeline.fit(x_train,y_train.values.reshape(-1)) \ny_pred=adav_boosting_pipeline.predict(x_test)\n\nprint(classification_report(y_test,y_pred))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T18:22:01.795959Z","iopub.execute_input":"2024-08-31T18:22:01.796374Z","iopub.status.idle":"2024-08-31T18:22:17.446984Z","shell.execute_reply.started":"2024-08-31T18:22:01.796335Z","shell.execute_reply":"2024-08-31T18:22:17.445873Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the baseline pipeline\nhist_grid_pipeline = Pipeline([\n    (\"standard scaling\",StandardScaler()),\n    (\"model\", HistGradientBoostingClassifier(class_weight=\"balanced\"))\n])\n\n# Evaluate the model\nhist_grid_pipeline.fit(x_train,y_train.values.reshape(-1)) \ny_pred=hist_grid_pipeline.predict(x_test)\n\nprint(classification_report(y_test,y_pred))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-31T18:41:53.448991Z","iopub.execute_input":"2024-08-31T18:41:53.449438Z","iopub.status.idle":"2024-08-31T18:41:56.521717Z","shell.execute_reply.started":"2024-08-31T18:41:53.449385Z","shell.execute_reply":"2024-08-31T18:41:56.520569Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# **Final Evaluation Pipeline**\n","metadata":{}},{"cell_type":"code","source":"# Conclusion:\n# Among the models tested, Random Forest emerged as the top performer, particularly excelling in handling class imbalance by achieving a strong balance between precision and recall for the minority class.\n# The Random Forest model provided a high F1-score for the minority class, making it the most reliable choice for this specific dataset.\n# HistGradientBoostingClassifier, while expected to perform well, struggled with low precision for the minority class, indicating potential overcompensation for the class imbalance. \n# Future work could involve further hyperparameter tuning for HistGradientBoosting to improve its precision, or exploring ensemble methods that combine the strengths of both Random Forest and HistGradientBoosting.","metadata":{},"outputs":[],"execution_count":null}]}